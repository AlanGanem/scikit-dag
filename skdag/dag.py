# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks_dev/dag.ipynb (unless otherwise specified).

__all__ = ['PipelineSession', 'rmdir', 'pickle_load', 'pickle_dump', 'recursive_dict', 'undefault_dict',
           'recursive_assign', 'folder_to_dict', 'dict_to_folder', 'BaseDAG', 'model_to_dot', 'DAGEstimator']

# Cell
from pathlib import Path
import d6tflow

import networkx as nx
from IPython import display

try:
    import pydot_ng as pydot
except:
    import pydot

from .node import BaseNode, _validate_name, input_task_factory

# Cell
class PipelineSession():
    '''
    sets d6tflow.settings.dirpath to pipeline name on open
    on close, resets d6tflow.settings.dirpath to default
    '''
    def __init__(self, pipeline_name):
        self.session_name = Path(pipeline_name)
        self.default_name = d6tflow.settings.dirpath
        return

    def __enter__(self,):
        d6tflow.settings.dirpath = self.session_name
        return

    def __exit__(self, exc_type, exc_value, traceback):
        d6tflow.settings.dirpath = self.default_name
        return

# Cell

def rmdir(directory):
    '''
    recursively remove dir and files in dir given by direcotry(path)
    '''
    directory = Path(directory)
    for item in directory.iterdir():
        if item.is_dir():
            rmdir(item)
        else:
            item.unlink()
    directory.rmdir()


def pickle_load(path, **kwargs):
    '''
    abstraction for loading object
    '''
    path = Path(path)
    try:
        with open(path.absolute(), 'rb') as f:
             obj = cloudpickle.load(f, **kwargs)

    except Exception as e:
        raise Exception(f'{str(e)} [{path.absolute()}]')

    return obj

def pickle_dump(obj, path, **kwargs):
    '''
    abstraction for dumping objects
    '''
    path = Path(path)
    try:
        with open(path.absolute(), 'wb') as f:
             cloudpickle.dump(obj, f, **kwargs)

    except Exception as e:
        raise Exception(f'{str(e)} [{path.absolute()}]')

    return


def recursive_dict():
    return defaultdict(recursive_dict)

def undefault_dict(dictionary):
    '''
    recursively transforms a default dicts into dicts
    '''
    for key in dictionary.keys():
        if isinstance(dictionary[key], defaultdict):
            dictionary[key] = undefault_dict(dict(dictionary[key]))

        elif isinstance(dictionary[key], dict):
            dictionary[key] = undefault_dict(dictionary[key])

        else:
            return dictionary

    dictionary = dict(dictionary)
    return dictionary

def recursive_assign(dictionary, keys, value):

    '''
    assigns value to dictinary[keys[0]][keys[1]]...[keys[n]] position
    '''

    if len(keys) == 1:
        dictionary[keys[0]] = value
        return dictionary

    else:
        dictionary[keys[0]] = recursive_assign(dictionary[keys[0]], keys[1:], value)

    return dictionary

def folder_to_dict(root_folder, patterns = ['*.pickle','*.pkl','*.sav']):

    '''
    creates a dict of unpickled objects found recursively under root folder and matching patterns
    '''

    obj_dict = {}
    root_folder = Path(root_folder)

    file_paths = []
    for pattern in patterns:
        file_paths += root_folder.rglob(pattern)

    for path in file_paths:
        obj = pickle_load(path)
        obj_dict[str(path)] = obj

    return obj_dict

def dict_to_folder(dictionary, path = '.', assert_new_folder = True, override = False):

    '''
    creates folder structure according to dictionary. Can ensure that no folders with the same name
    are found under path and to raise errors in case the same filenames already exists.
    '''
    root_path = Path(path)
    for key in dictionary:

        filepath = Path(key)
        if assert_new_folder:
            dirpath = root_path/filepath.parts[0]
            if dirpath.exists():
                raise FileExistsError(f'{dirpath.absolute()} already exists.')

        filepath = root_path/filepath
        if filepath.exists():
            if override:
                warn(f'{filepath.absolute()} already exists and will be overriden.')
            else:
                raise FileExistsError(f'{filepath.absolute()} already exists.')

        pickle_dump(dictionary[key], filepath)

    return

# Cell
def _traverse_upstream(output_node):
    '''
    traverses all nodes in the dag that ends at output_node.
    returns nodes and directed edges
    '''
    def __traverse_upstream(node, traversed_nodes = [], traversed_edges = []):

        if not isinstance(node, BaseNode):
            raise TypeError(f'Node should be instance of BaseNode, got {type(node)}')


        traversed_nodes.append(node)

        if not isinstance(node, InputTransformer): #keep traversing until reaches an InputNode

            for input_node in node.input_nodes:
                traversed_edges.append((input_node, node))
                __traverse_upstream(input_node, traversed_nodes)

        return traversed_nodes, traversed_edges

    traversed_nodes, traversed_edges = __traverse_upstream(output_node)
    return traversed_nodes, traversed_edges

# Cell
class BaseDAG(BaseNode):
    '''
    a base class containing a safe __getattr__ method
    to ensure that every method and attribute is accessed safely inside
    pipeline, i.e. all serialized files are dumped under the same root folder with pipeline name
    '''

    def __getattribute__(self, attr):
        '''
        returns a wrapped session method or attribute.
        if attribute is property, opertaions are run safely inside PipelineSession aswell
        '''

        name = super().__getattribute__('name')
        safe_run = super().__getattribute__('_safe_run')

        #safely get attribute value, useful in case attr is property
        attr_value = safe_run(super().__getattribute__, name)(attr)

        #case where attribute is method
        if callable(attr_value):
            attr_value = safe_run(attr_value, name)

        return attr_value

    def _safe_run(self, method, name):
        '''
        runs a method or retrieves an attribute safely inside a PipelineSession
        '''

        def session_wrapped_method(*args, **kwargs):
            with PipelineSession(pipeline_name = name):

                result = method(*args, **kwargs)

            return result

        return session_wrapped_method


# Cell
def model_to_dot(graph, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96, subgraph=False, **kwargs):


    def add_edge(dot, src, dst):
      if not dot.get_edge(src, dst):
        dot.add_edge(pydot.Edge(src, dst))

    #create dot object
    dot = pydot.Dot()
    dot.set('rankdir', rankdir)
    dot.set('concentrate', True)
    dot.set('dpi', dpi)
    dot.set_node_defaults(shape='record')
    for key in kwargs:
        dot.set(key, kwargs[key])


    for node in graph:

        if isinstance(node, InputTransformer):
            label = node.name
        else:
            label = f'{{{node.name} | {{{node.__class__.__name__} | {str(node.estimator).split("(")[0]}}} }}'

        dotnode = pydot.Node(node.name, label=label)
        dot.add_node(dotnode)

    for edge in graph.edges:
        add_edge(dot, edge[0].name, edge[1].name)

    return dot


# Cell
#TODO: define best name for class
class DAGEstimator(BaseDAG):

    def __init__(self, output_node, name):
        self.output_node = output_node
        self.name = _validate_name(name)

        #create node representaitons
        self._make_private_attributes()

        return

    def _make_private_attributes(self,):
        '''
        creates object private attributes
        '''
        #node attributes
        self._make_graph()
        self.__nodes = [node for node in self.graph]
        self.__nodes_dict = {node.name:node for node in self.graph}
        self.__input_nodes = tuple([i for i in self.graph if isinstance(i, InputTransformer)])
        self.__input_nodes_dict = {i.name:i for i in self.graph if isinstance(i, InputTransformer)}

        #palceholders for loaders and tasks
        self.__y_loader = None
        return

    def _make_graph(self):
        '''
        creates a graph (DAG) traversing output node until reaches inputs
        '''

        nodes,edges = _traverse_upstream(self.output_node)
        g = nx.DiGraph()
        g.add_nodes_from(nodes)
        g.add_edges_from(edges)
        self.__graph = g
        return

    @property
    def dirpath(self,):
        return Path('.')/self.name
    @property
    def nodes(self,):
        return self.__nodes
    @property
    def nodes_dict(self,):
        return self.__nodes_dict

    @property
    def graph(self,):
        return self.__graph

    @property
    def input_nodes(self,):
        return self.__input_nodes

    @property
    def input_nodes_dict(self,):
        return self.__input_nodes_dict

    @property
    def y_loader(self,):
        return self.__y_loader

    def _reset_data(self,):

        '''
        reset input of transform nodes
        '''

        #reset inputs and intermediate inputs of transform nodes
        for node in self.nodes:
            #reset input of transform nodes
            node._reset_transform_cache_in()

        #reset output of output node
        self.output_node._reset_transform_cache_out()

        #reset y
        self._reset_y_loader()


        #delete folders to ensure everything is deleted
        #delete all folders but "fit" which keeps the states
        delete_folders = list(set(self.dirpath.glob("*")) - set(self.dirpath.glob("fit")))
        for directory in delete_folders:
            rmdir(directory)

        return

    def _set_y_loader(self, y_loader):
        '''
        sets y_loader for NodeTransformers (skips InputTransformer)
        '''
        for node in self.__graph:
            #if not input node, assign y
            if isinstance(node, NodeTransformer):
                node._set_y_loader(y_loader)

    def _reset_y_loader(self,):
        '''
        resets y_loader for NodeTransformers (skips InputTransformer)
        '''
        for node in self.__graph:
            #if not input node, assign y
            if isinstance(node, NodeTransformer):
                node._reset_y_loader()

        return

    def _reset_estimators_states(self,):
        '''
        resets states (fitted objects) of nodes in DAG
        '''
        for node in self.nodes:
            #reset estimators states of nodes in DAG
            node._reset_fit_cache()

        #delete folders to ensure everything is deleted
        #delete all folders but "fit" which keeps the states
        delete_folders = self.dirpath.glob("fit")
        for directory in delete_folders:
            rmdir(directory)

        return

    def _check_graph(self,):

        if not nx.is_connected(nx.Graph(self.__graph)):
            raise Exception('DAG has disconected parts')


        if not nx.is_directed_acyclic_graph(self.__graph):
            raise Exception('Infered graph is not a DAG')


        repeated_names = set(
            [x.name for x in list(self.graph.nodes) if [i.name for i in self.graph.nodes].count(x.name) > 1])
        if repeated_names:
            raise NameError(f'Node names should be unique. Got repeated names: {repeated_names}')

        return True

    def _check_X(self, X):

        '''
        checks if X is valid input
        '''

        if isinstance(X, (tuple, list)):
            if len(X) != len(self.input_nodes):
                raise ValueError(f'DAG has {len(self.input_nodes)} input nodes, got {len(X)} inputs in X')

        elif isinstance(X, dict):
            if set(X) == set(self.input_nodes_dict):
                raise ValueError(f'DAG input node names are {set(self.input_nodes_dict)}, got {set(X)} keys in X')

        else:
            raise TypeError(f'X should be tuple or list of inputs or dict, got {type(X)}')

        return

    def _populate_input_nodes(self, X):
        '''
        populate input nodes with X passed (list or dict)
        '''
        if isinstance(X, (tuple, list)):
            for idx in range(len(self.input_nodes)):

                #TODO: define how to pass **kwargs for each input node
                self.input_nodes[idx].fit(X[idx])

        elif isinstance(X, dict):
            for key in range(len(self.input_nodes_dict)):
                self.input_nodes_dict[key].fit(X[key])

        else:
            raise TypeError(f'X should be list, tuple or dict, got {type(X)}')

        return

    def fit(self, X, y = None, **kwargs):


        '''
        X must be a list or dict pointing to input Nodes
        '''
        self._make_graph()
        self._check_graph()
        #check inputs
        self._check_X(X)
        # create y_loader
        y_loader = input_task_factory('y__' + self.name, y = y)

        #reset data
        self._reset_data()
        #reset transform and fit cache
        self._reset_estimators_states()

        #"populate" all nodes with y
        self._set_y_loader(y_loader)
        #populate input nodes with X
        self._populate_input_nodes(X)

        #recursively fit output node
        self.output_node._fit()
        return self

    def _infer(self, X, infer_method, reset_cache = True, **kwargs):

        '''
        abstract inference method, calls "infer_method" in the final node
        '''
        if reset_cache:
            #reset transform cache
            self._reset_data()

        #populate input nodes with X
        self._populate_input_nodes(X)

        #make infer task
        self.output_node._make_transform_task()

        #infer
        out = self.output_node._infer(None, infer_method, reset_cache, **kwargs)
        return out

    def transform(self, X, **kwargs):
        return self._infer(X, infer_method = 'transform', reset_cache = True, **kwargs)['X']

    def sub_dag(self, name):
        '''
        returns a new DAG instance, having its output node defined by name
        '''
        if name == self.name:
            raise NameError('sub_dag name should differ from parent dag name')

        return DAGEstimator(self.nodes_dict[name], name)

    def make_dot(self, **kwargs):
        '''
        creates a dot object of dag accroding to model_to_dot function
        '''
        return model_to_dot(self.graph, **kwargs)

    def show_dot(self, **kwargs):
        '''
        creates dot file of the dag, prints png and returns dot object
        '''
        return display.Image(self.make_dot(**kwargs).create_png())

    def _create_states_directory(self,):
        '''
        creates directory with fitted estimators from some dict
        '''
    def __reppr__(self,):
        return self.name

    def _reduce_dag():
        '''
        creates a dictionary with all reference for reconstructing the dag (for serializing purposes)
        '''
        return

    def _reduce_states():
        '''
        creates a dict of objects, with their relative paths as keys , to pickle pipeline
        '''
        #node.fit_task().output().path <--- obj paths
