{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: port to dask:\n",
    "# dag class will define the nx and dask graph, will define functions (fit(estimator, X, y), transform(estimator, X)) to be run in\n",
    "#dask graph. dag building will keep the same interface (with nodes), but node class will be much simpler, since all cache\n",
    "#handling is done internally in dask graph\n",
    "# Dask Delayed or Dask Graph? i guess delayed is simpler, just pass object methods inside delayed, and create fit/transform blocks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#workaround to make relative imports inside notebook\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from warnings import warn\n",
    "from shutil import rmtree\n",
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "from IPython import display\n",
    "\n",
    "try:\n",
    "    import pydot_ng as pydot\n",
    "except:\n",
    "    import pydot\n",
    "\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "    \n",
    "from skdag.node import NodeEstimator, Input, Target, BaseInputNode, _validate_name\n",
    "from skdag.utils import remove_folder_or_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PipelineSession\n",
    "> Session handler to safely serialize objects from pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# serialize and deserialize functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def pickle_load(path, **kwargs):\n",
    "    '''\n",
    "    abstraction for loading object\n",
    "    '''\n",
    "    path = Path(path)\n",
    "    try:\n",
    "        with open(path.absolute(), 'rb') as f:\n",
    "             obj = cloudpickle.load(f, **kwargs)\n",
    "        \n",
    "    except Exception as e:            \n",
    "        raise Exception(f'{str(e)} [{path.absolute()}]')\n",
    "    \n",
    "    return obj\n",
    "\n",
    "def pickle_dump(obj, path, **kwargs):\n",
    "    '''\n",
    "    abstraction for dumping objects\n",
    "    '''\n",
    "    path = Path(path)\n",
    "    try:\n",
    "        with open(path.absolute(), 'wb') as f:\n",
    "             cloudpickle.dump(obj, f, **kwargs)                            \n",
    "        \n",
    "    except Exception as e:            \n",
    "        raise Exception(f'{str(e)} [{path.absolute()}]')\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def recursive_dict():\n",
    "    return defaultdict(recursive_dict)\n",
    "\n",
    "def undefault_dict(dictionary):\n",
    "    '''\n",
    "    recursively transforms a default dicts into dicts\n",
    "    '''\n",
    "    for key in dictionary.keys():\n",
    "        if isinstance(dictionary[key], defaultdict):\n",
    "            dictionary[key] = undefault_dict(dict(dictionary[key]))\n",
    "        \n",
    "        elif isinstance(dictionary[key], dict):\n",
    "            dictionary[key] = undefault_dict(dictionary[key])\n",
    "        \n",
    "        else:            \n",
    "            return dictionary\n",
    "    \n",
    "    dictionary = dict(dictionary)\n",
    "    return dictionary\n",
    "            \n",
    "def recursive_assign(dictionary, keys, value):\n",
    "    \n",
    "    '''\n",
    "    assigns value to dictinary[keys[0]][keys[1]]...[keys[n]] position    \n",
    "    '''\n",
    "    \n",
    "    if len(keys) == 1:\n",
    "        dictionary[keys[0]] = value\n",
    "        return dictionary\n",
    "    \n",
    "    else:\n",
    "        dictionary[keys[0]] = recursive_assign(dictionary[keys[0]], keys[1:], value)\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "def folder_to_dict(root_folder, patterns = ['*.pickle','*.pkl','*.sav']):\n",
    "    \n",
    "    '''\n",
    "    creates a dict of unpickled objects found recursively under root folder and matching patterns\n",
    "    '''\n",
    "        \n",
    "    obj_dict = {}\n",
    "    root_folder = Path(root_folder)\n",
    "    \n",
    "    file_paths = []\n",
    "    for pattern in patterns:\n",
    "        file_paths += root_folder.rglob(pattern)\n",
    "    \n",
    "    for path in file_paths:        \n",
    "        obj = pickle_load(path)\n",
    "        obj_dict[str(path)] = obj\n",
    "    \n",
    "    return obj_dict\n",
    "\n",
    "def dict_to_folder(dictionary, path = '.', assert_new_folder = True, override = False):\n",
    "    \n",
    "    '''\n",
    "    creates folder structure according to dictionary. Can ensure that no folders with the same name \n",
    "    are found under path and to raise errors in case the same filenames already exists.\n",
    "    '''\n",
    "    root_path = Path(path)\n",
    "    for key in dictionary:\n",
    "        \n",
    "        filepath = Path(key)        \n",
    "        if assert_new_folder:\n",
    "            dirpath = root_path/filepath.parts[0]\n",
    "            if dirpath.exists():\n",
    "                raise FileExistsError(f'{dirpath.absolute()} already exists.')        \n",
    "        \n",
    "        filepath = root_path/filepath\n",
    "        if filepath.exists():\n",
    "            if override:\n",
    "                warn(f'{filepath.absolute()} already exists and will be overriden.')                \n",
    "            else:\n",
    "                raise FileExistsError(f'{filepath.absolute()} already exists.')\n",
    "        \n",
    "        pickle_dump(dictionary[key], filepath)\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _traverse_upstream(output_node):  \n",
    "    '''\n",
    "    traverses all nodes in the dag that ends at output_node.\n",
    "    returns nodes and directed edges\n",
    "    '''\n",
    "    def __traverse_upstream(node, traversed_nodes = [], traversed_edges = []):\n",
    "        \n",
    "        if not isinstance(node, (NodeEstimator, Input)):\n",
    "            raise TypeError(f'Node should be instance of NodeEstimator or Input, got {type(node)}')\n",
    "        \n",
    "        node = Cacher(node, f'{node.name}.pkl') #append Cacher to graph\n",
    "        traversed_nodes.append(node)        \n",
    "        \n",
    "        if not isinstance(node.get(), BaseInputNode): #keep traversing until reaches an InputNode\n",
    "            if not node.get().input_nodes:                \n",
    "                raise ValueError(f'{node.get().name} input nodes are empty. Populate {node.get().name} by calling the object and passing input_nodes')\n",
    "            for input_node in node.get().input_nodes:\n",
    "                #make input node a Cacher\n",
    "                input_node = Cacher(input_node, f'{input_node.name}.pkl')\n",
    "                traversed_edges.append((input_node, node))\n",
    "                __traverse_upstream(input_node, traversed_nodes)        \n",
    "\n",
    "        return traversed_nodes, traversed_edges\n",
    "    \n",
    "    traversed_nodes, traversed_edges = __traverse_upstream(output_node)\n",
    "    return traversed_nodes, traversed_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseDag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BaseDAG(BaseEstimator):\n",
    "    pass\n",
    "\n",
    "#class BaseDAG(BaseNode):\n",
    "#    '''\n",
    "#    a base class containing a safe __getattr__ method\n",
    "#    to ensure that every method and attribute is accessed safely inside \n",
    "#    pipeline, i.e. all serialized files are dumped under the same root folder with pipeline name\n",
    "#    '''\n",
    "#    \n",
    "#    def __getattribute__(self, attr):\n",
    "#        '''\n",
    "#        returns a wrapped session method or attribute.\n",
    "#        if attribute is property, opertaions are run safely inside PipelineSession aswell\n",
    "#        '''        \n",
    "#        \n",
    "#        name = super().__getattribute__('name')\n",
    "#        safe_run = super().__getattribute__('_safe_run')        \n",
    "#        \n",
    "#        #safely get attribute value, useful in case attr is property\n",
    "#        attr_value = safe_run(super().__getattribute__, name)(attr)\n",
    "#        \n",
    "#        #case where attribute is method\n",
    "#        if callable(attr_value):\n",
    "#            attr_value = safe_run(attr_value, name)\n",
    "#                    \n",
    "#        return attr_value\n",
    "#    \n",
    "#    def _safe_run(self, method, name):\n",
    "#        '''\n",
    "#        runs a method or retrieves an attribute safely inside a PipelineSession\n",
    "#        '''        \n",
    "#        \n",
    "#        def session_wrapped_method(*args, **kwargs):\n",
    "#            with PipelineSession(pipeline_name = name):                \n",
    "#                \n",
    "#                result = method(*args, **kwargs)\n",
    "#                \n",
    "#            return result\n",
    "#\n",
    "#        return session_wrapped_method                    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphviz dot utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def model_to_dot(graph, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96, subgraph=False, **kwargs):\n",
    "\n",
    "    \n",
    "    def add_edge(dot, src, dst):\n",
    "      if not dot.get_edge(src, dst):\n",
    "        dot.add_edge(pydot.Edge(src, dst))\n",
    "\n",
    "    #create dot object\n",
    "    dot = pydot.Dot()\n",
    "    dot.set('rankdir', rankdir)\n",
    "    dot.set('concentrate', True)\n",
    "    dot.set('dpi', dpi)\n",
    "    dot.set_node_defaults(shape='record')\n",
    "    for key in kwargs:\n",
    "        dot.set(key, kwargs[key])\n",
    "    \n",
    "\n",
    "    for node in graph:\n",
    "        \n",
    "        if isinstance(node, Input):\n",
    "            label = node.name\n",
    "        else:                                    \n",
    "            if node.frozen:\n",
    "                name = node.name + ' (Frozen)'\n",
    "            else:\n",
    "                name = node.name\n",
    "            label = f'{{{name} | {{{node.__class__.__name__} | {str(node.estimator).split(\"(\")[0]}}} }}'\n",
    "        \n",
    "        dotnode = pydot.Node(node.name, label=label)\n",
    "        dot.add_node(dotnode)\n",
    "    \n",
    "    for edge in graph.edges:\n",
    "        add_edge(dot, edge[0].name, edge[1].name)\n",
    "\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAGEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.base import clone\n",
    "from copy import deepcopy\n",
    "from sklearn.exceptions import NotFittedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO: define best name for class\n",
    "class DAGEstimator(BaseDAG):\n",
    "    \n",
    "    def __init__(self, output_node, name, dirpath = './_skdag_cache'):        \n",
    "        \n",
    "        self.name = self._validate_name(name)  \n",
    "        self.dirpath = Path(dirpath)\n",
    "        #create node representaitons\n",
    "        self._make_private_attributes(output_node)        \n",
    "        \n",
    "        self.output_node = [i for i in self.__graph if i.name == f'{self.name}__{output_node.name}'][0]        \n",
    "        \n",
    "        return                                \n",
    "    \n",
    "    def _validate_name(self, name):\n",
    "        return name\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.nodes_dict[item]\n",
    "        \n",
    "    def _make_private_attributes(self, output_node):\n",
    "        '''\n",
    "        creates object private attributes\n",
    "        '''        \n",
    "        #node attributes\n",
    "        graph = self._make_graph(output_node, make_clones = True)\n",
    "        self.__graph = self._clone_graph(graph)\n",
    "        #make names with pipename prefix\n",
    "        [setattr(node, 'name', f'{self.name}__' + node.name) for node in self.__graph]\n",
    "        self.__nodes = [node for node in self.__graph]\n",
    "        self.__nodes_dict = {node.name:node for node in self.__graph}\n",
    "        self.__input_nodes = tuple([i for i in self.__graph if isinstance(i, Input)])\n",
    "        self.__input_nodes_dict = {i.name:i for i in self.__graph if isinstance(i, Input)}        \n",
    "                \n",
    "        return\n",
    "    \n",
    "    def _make_graph(self, output_node, make_clones = True):                \n",
    "        '''\n",
    "        creates a graph (DAG) traversing output node until reaches inputs\n",
    "        '''\n",
    "        \n",
    "        nodes,edges = _traverse_upstream(output_node)\n",
    "        g = nx.DiGraph()\n",
    "        g.add_nodes_from(nodes)\n",
    "        g.add_edges_from(edges)                                \n",
    "        return g\n",
    "    \n",
    "    def _clone_graph(self, graph):        \n",
    "        clones = {}\n",
    "        for node in nx.algorithms.dag.topological_sort(graph):\n",
    "            clones[node.name] = deepcopy(node)\n",
    "        \n",
    "        output_clone = clones[node.name]                                                     \n",
    "        #replace nodes by its clones\n",
    "        for node in graph:\n",
    "            if not isinstance(node, Input):\n",
    "                node = node(*[clones[child.name] for child in node.input_nodes])\n",
    "        \n",
    "        graph = self._make_graph(output_clone, make_clones = False)\n",
    "        return graph\n",
    "    \n",
    "    def _make_inputs_dict(self, X, y):\n",
    "        '''\n",
    "        creates dict of (node.name,Input) and (node.name,Target) pairs\n",
    "        returns dicts of X, y        \n",
    "        '''\n",
    "        return X, y\n",
    "        \n",
    "    \n",
    "    def _populate_input_nodes(self, X, y):\n",
    "        '''\n",
    "        X and y should be a dict of node.name:value\n",
    "        '''\n",
    "        \n",
    "        X, y = self._make_inputs_dict(X, y)\n",
    "        \n",
    "        for node in nx.algorithms.dag.topological_sort(self.graph):        \n",
    "            #populate inputs and static targets\n",
    "            \n",
    "            if isinstance(node, Input):\n",
    "                node.fit(X[node.name])\n",
    "                continue\n",
    "            \n",
    "            if isinstance(node, Target):\n",
    "                node.fit(y[node.name])\n",
    "                continue\n",
    "        return                \n",
    "    \n",
    "    def _dask_fit_pipeline(self, inputs, targets, **kwargs):\n",
    "        \n",
    "        '''\n",
    "        inputs and targets should be a dict of (node.name,Input/Target) pairs\n",
    "        '''\n",
    "        \n",
    "        outputs = {}\n",
    "        targets = {}\n",
    "        estimators = {}\n",
    "        for node in nx.algorithms.dag.topological_sort(self.graph):\n",
    "                                \n",
    "            #populate inputs and targets. assume they are already fitted/populated\n",
    "            if isinstance(node, Input):\n",
    "                outputs[node.name] = delayed(node.transform)()\n",
    "            \n",
    "            if isinstance(node, Target):\n",
    "                outputs[node.name] = delayed(node.transform)()\n",
    "            \n",
    "            if isinstance(node, NodeEstimator): #ignore Input and Target nodes\n",
    "                                                                 \n",
    "                X = (outputs[p.name] for p in node.input_nodes) #delayed object\n",
    "                y = targets[node.target_node.name] #also delayed object\n",
    "                estimators[node.name] = delayed(node.fit)(X,y)            \n",
    "                outputs[node.name] = delayed(estimators[node.name].transform)(X)\n",
    "        \n",
    "        return estimators\n",
    "    \n",
    "    def _dask_transform_pipeline(self, inputs, targets, output_node = None, **kwargs):\n",
    "        \n",
    "        '''\n",
    "        inputs should be a dict of (node.name,Input) pairs\n",
    "        '''\n",
    "        if output_node is None:\n",
    "            output_node = self.output_node\n",
    "        \n",
    "        outputs = {}        \n",
    "        for node in nx.algorithms.dag.topological_sort(self.graph):                                \n",
    "                                \n",
    "            #populate inputs and targets. assume they are already fitted/populated\n",
    "            if isinstance(node, Input):\n",
    "                outputs[node.name] = delayed(node.transform)()\n",
    "            \n",
    "            if isinstance(node, Target):\n",
    "                outputs[node.name] = delayed(node.transform)()\n",
    "            \n",
    "            if isinstance(node, NodeEstimator): #ignore Input and Target nodes\n",
    "                                                                 \n",
    "                X = (outputs[p.name] for p in node.input_nodes) #delayed object                                \n",
    "                outputs[node.name] = delayed(node.transform)(X)\n",
    "        \n",
    "        return outputs                \n",
    "    \n",
    "    @property\n",
    "    def dirpath(self,):\n",
    "        return d6tflow.settings.dirpath/self.name\n",
    "    @property\n",
    "    def nodes(self,):\n",
    "        return self.__nodes\n",
    "    @property\n",
    "    def nodes_dict(self,):\n",
    "        return self.__nodes_dict\n",
    "    \n",
    "    @property\n",
    "    def graph(self,):\n",
    "        return self.__graph\n",
    "    \n",
    "    @property\n",
    "    def input_nodes(self,):\n",
    "        return self.__input_nodes\n",
    "    \n",
    "    @property\n",
    "    def input_nodes_dict(self,):\n",
    "        return self.__input_nodes_dict\n",
    "    \n",
    "    @property\n",
    "    def y_loader(self,):\n",
    "        return self.__y_loader                                    \n",
    "    \n",
    "    def _reset_data(self,):\n",
    "        \n",
    "        '''\n",
    "        reset input of transform nodes\n",
    "        '''\n",
    "                \n",
    "        #reset inputs and intermediate inputs of transform nodes\n",
    "        for node in self.nodes:\n",
    "            if not node.output_path is None:\n",
    "                #reset input of transform nodes            \n",
    "                remove_folder_or_file(node.output_path)\n",
    "\n",
    "        #reset y\n",
    "        self._reset_y_loader()                                        \n",
    "        return\n",
    "    \n",
    "    def _set_y_loader(self, y):\n",
    "        '''\n",
    "        sets y_loader for NodeTransformers (skips Input)\n",
    "        '''\n",
    "        \n",
    "        if isinstance(y, dict):            \n",
    "            \n",
    "            y = {self.name + f'__{k}':v for k,v in y.items()}\n",
    "            passed_keys = set(y)\n",
    "            existing_keys = set(self.nodes_dict)\n",
    "            wrong_passed_keys = passed_keys - existing_keys \n",
    "            if wrong_passed_keys:\n",
    "                raise ValueError(f'Unknown node names: {wrong_passed_keys}')\n",
    "                \n",
    "            #create input tasks\n",
    "            default_loader = input_task_factory(self.name + f'__None', y = None)\n",
    "            y_loader = {}\n",
    "            for key in y:        \n",
    "                y_loader[key] = input_task_factory(f'{key}__target', y = y[key])\n",
    "            \n",
    "            for node in self.__graph:\n",
    "                #if not input node, assign y\n",
    "                if isinstance(node, NodeTransformer):                \n",
    "                    if node.name in y_loader:\n",
    "                        node._set_y_loader(y_loader[node.name])\n",
    "                    else:\n",
    "                        node._set_y_loader(default_loader)\n",
    "                        \n",
    "        else:\n",
    "            \n",
    "            y_loader = input_task_factory(self.name + '__target', y = y)\n",
    "            for node in self.__graph:\n",
    "                #if not input node, assign y                \n",
    "                if isinstance(node, NodeTransformer):                \n",
    "                    node._set_y_loader(y_loader)\n",
    "                    \n",
    "        return\n",
    "\n",
    "    def _reset_y_loader(self,):\n",
    "        '''\n",
    "        resets y_loader for NodeTransformers (skips Input)\n",
    "        '''\n",
    "        try:\n",
    "            self.y_loader().output().path.unlink()\n",
    "        except Exception as e:\n",
    "            warn(str(e))\n",
    "            pass\n",
    "                \n",
    "        return\n",
    "            \n",
    "    def _reset_estimators_states(self,):\n",
    "        '''\n",
    "        resets states (fitted objects) of nodes in DAG\n",
    "        '''\n",
    "        for node in self.nodes:\n",
    "            if not isinstance(node, Input):\n",
    "                if not node.estimator_path is None:                                    \n",
    "                    remove_folder_or_file(node.estimator_path)\n",
    "                    \n",
    "\n",
    "        return\n",
    "    \n",
    "    def _check_graph(self,):\n",
    "        \n",
    "        if not nx.is_connected(nx.Graph(self.__graph)):\n",
    "            raise Exception('DAG has disconected parts')\n",
    "        \n",
    "        \n",
    "        if not nx.is_directed_acyclic_graph(self.__graph):\n",
    "            raise Exception('Infered graph is not a DAG')\n",
    "        \n",
    "        \n",
    "        repeated_names = set(\n",
    "            [x.name for x in list(self.graph.nodes) if [i.name for i in self.graph.nodes].count(x.name) > 1])\n",
    "        if repeated_names:\n",
    "            raise NameError(f'Node names should be unique. Got repeated names: {repeated_names}')\n",
    "        \n",
    "        return True    \n",
    "    \n",
    "    def _check_X(self, X):\n",
    "        \n",
    "        '''\n",
    "        checks if X is valid input\n",
    "        '''\n",
    "        \n",
    "        if isinstance(X, (tuple, list)):\n",
    "            if len(X) != len(self.input_nodes):\n",
    "                raise ValueError(f'DAG has {len(self.input_nodes)} input nodes, got {len(X)} inputs in X')\n",
    "        \n",
    "        elif isinstance(X, dict):                        \n",
    "            if set(X) == set(self.input_nodes_dict):\n",
    "                raise ValueError(f'DAG input node names are {set(self.input_nodes_dict)}, got {set(X)} keys in X')\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(f'X should be tuple or list of inputs or dict, got {type(X)}')\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def _populate_input_nodes(self, X):\n",
    "        '''\n",
    "        populate input nodes with X passed (list or dict)\n",
    "        '''\n",
    "        if isinstance(X, (tuple, list)):\n",
    "            for idx in range(len(self.input_nodes)):\n",
    "                \n",
    "                #TODO: define how to pass **kwargs for each input node\n",
    "                self.input_nodes[idx].fit(X[idx])\n",
    "        \n",
    "        elif isinstance(X, dict):\n",
    "            for key in range(len(self.input_nodes_dict)):\n",
    "                self.input_nodes_dict[key].fit(X[key])\n",
    "                \n",
    "        else:\n",
    "            raise TypeError(f'X should be list, tuple or dict, got {type(X)}')\n",
    "        \n",
    "        return                    \n",
    "    \n",
    "    def _clear_pipeline(self,):\n",
    "        '''\n",
    "        removes all cached files in Pipeline folder\n",
    "        '''\n",
    "        remove_folder_or_file(self.dirpath)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y = None, **kwargs):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        X must be a list or dict pointing to input Nodes\n",
    "        '''\n",
    "        #self._make_graph() why create graph again?        \n",
    "        self._check_graph()\n",
    "        #check inputs        \n",
    "        self._check_X(X)        \n",
    "        #clear states and data\n",
    "        self._clear_pipeline()\n",
    "        #\"populate\" all nodes with y\n",
    "        self._set_y_loader(y)        \n",
    "        #populate input nodes with X\n",
    "        self._populate_input_nodes(X)                            \n",
    "        \n",
    "        #recursively fit output node                            \n",
    "        self.output_node._fit()        \n",
    "        return self                \n",
    "    \n",
    "    def _infer(self, X, infer_method, reset_cache = True, **kwargs):\n",
    "        \n",
    "        '''\n",
    "        abstract inference method, calls \"infer_method\" in the final node\n",
    "        '''\n",
    "        if reset_cache:\n",
    "            #reset transform cache\n",
    "            self._reset_data()\n",
    "\n",
    "        #populate input nodes with X\n",
    "        self._populate_input_nodes(X)\n",
    "        \n",
    "        #make infer task\n",
    "        self.output_node._make_transform_task()\n",
    "        \n",
    "        #infer             \n",
    "        out = self.output_node._infer(None, infer_method, **kwargs)        \n",
    "        return out                \n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        return self._infer(X, infer_method = 'transform', reset_cache = True, **kwargs)['X']\n",
    "    \n",
    "    def sub_dag(self, name):\n",
    "        '''\n",
    "        returns a new DAG instance, having its output node defined by name\n",
    "        '''\n",
    "        if name == self.name:\n",
    "            raise NameError('sub_dag name should differ from parent dag name')\n",
    "        \n",
    "        return DAGEstimator(self.nodes_dict[name], name)\n",
    "    \n",
    "    def make_dot(self, **kwargs):\n",
    "        '''\n",
    "        creates a dot object of dag accroding to model_to_dot function\n",
    "        '''\n",
    "        return model_to_dot(self.graph, **kwargs)                \n",
    "    \n",
    "    def show_dot(self, **kwargs):\n",
    "        '''\n",
    "        creates dot file of the dag, prints png and returns dot object\n",
    "        ''' \n",
    "        return display.Image(self.make_dot(**kwargs).create_png())\n",
    "        \n",
    "    def _create_states_directory(self,):\n",
    "        '''\n",
    "        creates directory with fitted estimators from some dict\n",
    "        '''\n",
    "    def __reppr__(self,):\n",
    "        return self.name\n",
    "    \n",
    "    def __getstate__(self,):\n",
    "        '''\n",
    "        creates a dictionary with all reference for reconstructing the dag (for serializing purposes)\n",
    "        '''\n",
    "        artifacts = {}\n",
    "        for node in self.nodes:\n",
    "            if isinstance(node, NodeTransformer):\n",
    "                if node.cached_estimator:\n",
    "                    node.estimator = node.estimator.load()\n",
    "                artifacts[node.fit_task().output().path] = node.fit_task().output().load()\n",
    "        \n",
    "        self.__serializable_artifacts = artifacts\n",
    "        return self.__dict__\n",
    "    \n",
    "    def __setstate__(self, d):\n",
    "        '''\n",
    "        creates a dict of objects, with their relative paths as keys , to pickle pipeline\n",
    "        '''\n",
    "        self.__dict__ = d\n",
    "        \n",
    "        for path in d['__serializable_artifacts']:\n",
    "            #create empty directories\n",
    "            if not path.parent.exists():\n",
    "                path.parent.mkdir(parents = True)\n",
    "            \n",
    "            if path.exists():\n",
    "                raise FileExistsError(f'{path.absolute()} already exists.')\n",
    "            with open(path, 'wb') as f:                \n",
    "                cloudpickle.dump(self.__serializable_artifacts[path], f)\n",
    "            \n",
    "        self.__serializable_artifacts = None\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAGEstimator Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from skdag.node import NodeEstimator, Input, ConcatenateNode\n",
    "import numpy as np\n",
    "\n",
    "#custom defined function\n",
    "from functools import partial\n",
    "def npsum(axis = 0):     \n",
    "    return partial(np.sum, axis = axis)\n",
    "\n",
    "in1 = Input()\n",
    "in2 = Input()\n",
    "in3 = Input()\n",
    "\n",
    "\n",
    "node1 = NodeEstimator(FunctionTransformer(npsum(axis = 0)))(in2,in1)\n",
    "\n",
    "concat1 = ConcatenateNode('Concat1')(node1,in3)\n",
    "node2 = NodeEstimator(LinearRegression(), 'model1', transform_method = 'predict')(concat1)\n",
    "\n",
    "concat2 = ConcatenateNode('Concat2')(node1,node2,in1, in3)\n",
    "node3 = NodeEstimator(RandomForestRegressor(), 'model2', transform_method = 'predict')(concat2)\n",
    "\n",
    "concat3 = ConcatenateNode('concat_final')(node1,node3, in2, in3)\n",
    "node4 = NodeEstimator(FunctionTransformer(partial(np.mean, axis = 1)), 'sum_final')(concat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(1000,3)\n",
    "y = np.random.randn(1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAGEstimator(node4, 'Pipeline2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag.fit([X,X,X], {'model1':y, 'model2':y+100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag.nodes[0].freeze()\n",
    "dag.show_dot(dpi = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag.nodes[0].unfreeze()\n",
    "dag.show_dot(dpi = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(1000000,3)\n",
    "results = dag.transform([X,X,X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(results)\n",
    "sns.distplot(X.mean(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
